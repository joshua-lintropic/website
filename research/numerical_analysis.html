<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Numerical Analysis â€” Joshua Lin</title>

    <link rel="shortcut icon" href="../assets/images/logo.ico" type="image/x-icon">

    <link rel="stylesheet" href="../assets/css/style.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">

    <script> window.MathJax = { chtml: { scale: 0.9 }, tex: { inlineMath: [['$', '$']] } }; </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.assetsPath = '..';</script>
</head>

<body>
    <main>
        <div class="main-content">

            <article class="about active" data-page="blog-post">
                <header>
                    <h2 class="h2 article-title">Numerical Analysis</h2>
                </header>
                <section class="about-text">

                    <p>
                        <i>Notes on convergence, stability, rootfinding, floating-point analysis, linear systems,
                            optimization, and eigenstuff.</i>
                    </p>

                    <br>
                    <header>
                        <h3>Convergence and Stability</h3>
                    </header>

                    <br>
                    <header>
                        <h4>Rates of Convergence</h4>
                    </header>

                    <p>
                        In the following, we adopt the convention $e_{k} := |x^{*} - x_{k}|$, where $x^{*}$ is the
                        desired value and $x_{k}$ is the $k^{\text{th}}$ iterate.
                    </p>

                    <p>
                        <i>Linear Convergence.</i> Iterates $\{ x_{k} \}_{k}$ converge linearly with rate $\rho$ if
                    </p>

                    $$
                    \lim_{ k \to \infty } \frac{e_{k+1}}{e_{k}} = \rho < 1 $$ <p>
                        <i>Superlinear Convergence.</i> Iterates $\{ x_{k} \}_{k}$ converge superlinearly with order
                        $p>1$ if
                        </p>

                        $$
                        \lim_{ k \to \infty } \frac{e_{k+1}}{e_{k}^{p}} = M < 1 $$ <p>
                            <i>Quadratic Convergence.</i> Iterates $\{ x_{k} \}_{k}$ converge quadratically if
                            </p>

                            $$
                            \lim_{ k \to \infty } \frac{e_{k+1}}{e_{k}^{2}} = M < \infty $$ <br>
                                <header>
                                    <h4>Numerical Stability</h4>
                                </header>

                                <p>
                                    <i>Forward Stability.</i> Let $f$ be an algorithm and $\tilde{f}$ be its floating
                                    point approximation. Then $f$ is forward stable if
                                </p>

                                $$
                                \frac{\|f(x) - \tilde{f}(x)\|}{\|f(x)\|} = \mathcal{O}(\varepsilon_{\text{mach}})
                                $$

                                <p>
                                    <i>Backwards Stable.</i> Let $f$ be an algorithm and $\tilde{f}$ be its floating
                                    point approximation. Then $f$ is backward stable if
                                </p>

                                $$
                                \frac{\|x-\tilde{x}\|}{\|x\|} = \mathcal{O}(\varepsilon_{\text{mach}}),
                                \quad \tilde{f}(x) = f(\tilde{x})
                                $$

                                <div class="callout callout-note">
                                    <div class="callout-title">Intuition: Forward Stability</div>
                                    <div class="callout-content">
                                        Forward stability is "almost the right solution for the right problem", while
                                        backward stability is "the right solution for almost the right problem."
                                    </div>
                                </div>

                                <br>
                                <header>
                                    <h3>Rootfinding</h3>
                                </header>

                                <br>
                                <header>
                                    <h4>Bisection Search</h4>
                                </header>

                                <p>
                                    <b>Theorem (Existence of A Root).</b> Let $f:[a,b]\to \mathbb{R}$ be a continuous
                                    function. If $f(a) f(b) \leq 0$, then there exists a root $x^{*}\in[a,b]$.
                                </p>

                                <p>
                                    <i>Proof.</i> Apply the intermediate value theorem. $\blacksquare$
                                </p>

                                <p>
                                    <b>Bisection Search.</b> Given continuous $f:[a,b]\to \mathbb{R}$ with $f(a)f(b)
                                    \leq 0$, bisection iteratively updates the feasible region with the midpoint of the
                                    existing region. The convergence is linear.
                                </p>

                                <pre><code class="language-python">def bisection_search(f, a, b, tol=1e-12): 
    """
    Arguments: 
        f: continuous function on [a,b]
        a, b: reals satisfying f(a)f(b) <= 0

    Returns:
        A root of f in [a,b]
    """
    if f(a) * f(b) > 0:
        raise ValueError("f(a)f(b) must be non-negative")

    while abs(f(a)) > tol:
        c = (a + b) / 2
        if f(a) * f(c) <= 0:
            b = c
        else:
            a = c

    return a</code></pre>

                                <br>
                                <header>
                                    <h4>Quasi-Newton Methods</h4>
                                </header>

                                <p>
                                    <b>Newton's Method.</b> Let $f \in C^{2}(x^{*}\pm d)$ where $x^{*}$ is a simple
                                    root. Suppose
                                </p>

                                $$
                                \left| \frac{f''(x)}{f'(y)} \right|
                                \leq K
                                \quad
                                (x,y \in [x^{*}\pm d])
                                $$

                                <p>
                                    for some $K>0$. If $x_{0} \in (x^{*}\pm h)$ where $h=\min\left( d, \frac{1}{K}
                                    \right)$, then Newton's iterates
                                </p>

                                $$
                                x_{k+1} = x_{k} - \frac{f(x_{k})}{f'(x_{k})}
                                $$

                                <p>
                                    converges quadratically.
                                </p>

                                <div class="callout callout-note">
                                    <div class="callout-title">Intuition: Newton's Method</div>
                                    <div class="callout-content">
                                        Newton's approximates the function with a tangent line at each iteration. It
                                        thus converges when the function is "approximately linear", i.e. the curvature
                                        $f''$ is small (bounded) relative to its slope $f'$. The iterations come from
                                        $0\approx f(x_{0})+f'(x_{0})(x - x_{0})$.
                                    </div>
                                </div>

                                <p>
                                    <b>Secant Method.</b> In the context of Newton's method, suppose the derivatives
                                    $f'$ are not available. Using finite-difference approximations yields iterates
                                </p>

                                $$
                                x_{k+1} = x_{k} - \frac{x_{k} - x_{k-1}}{f(x_{k}) - f(x_{k-1})} f(x_{k})
                                $$

                                <p>
                                    which converges superlinearly with order $\frac{1+\sqrt{ 5 }}{2} \approx 1.6$.
                                </p>

                                <p>
                                    <b>Broyden's Method.</b> Generalizes quasi-Newton methods for higher dimensional
                                    domains.
                                </p>

                                <br>
                                <header>
                                    <h4>Fixed-Point Iterations</h4>
                                </header>

                                <p>
                                    <i>Lemma</i>. If $g$ is continuous and $x_{k+1} = g(x_{k})$ converges to $x^{*}$
                                    then $x^{*}$ is a fixed point.
                                </p>

                                <p>
                                    <b>Theorem (Fixed Point).</b> If $g\in C([a,b] \to [a,b])$ then there exists $x^{*}
                                    \in[a,b]$. Furthermore, if $g'$ exists and $\|g'\|_{\infty} \leq \rho < 1$ then the
                                        fixed-point iterations converge linearly with rate $\rho$, and the fixed point
                                        is unique. </p>

                                        <p>
                                            <i>Proof.</i> For existence, apply the Intermediate Value Theorem. For
                                            convergence and uniqueness, apply first-order Taylor expansion.
                                            $\blacksquare$
                                        </p>

                                        <div class="callout callout-note">
                                            <div class="callout-title">Contraction Mapping Theorem</div>
                                            <div class="callout-content">
                                                This theorem generalizes to general complete metric spaces via the
                                                Banach fixed-point theorem (also known as the Contraction Mapping
                                                Theorem).
                                            </div>
                                        </div>

                                        <br>
                                        <header>
                                            <h3>Floating-Point Analysis</h3>
                                        </header>

                                        <br>
                                        <header>
                                            <h4>Representation</h4>
                                        </header>

                                        <p>
                                            <i>Floating Point Representation.</i> A <i>normal</i> FPN has form
                                        </p>

                                        $$
                                        (-1)^{s} \times (1.b_{1}b_{2}\dots b_{p})_{2} \times 2^{E}
                                        $$

                                        <p>
                                            Here, $s \in \{ 0,1 \}$ is the signed bit, $E$ is the exponent,
                                            $1.b_{1}b_{2}\dots b_{p}$ is the significand.
                                        </p>
                                        <ul>
                                            <li>64-bit double precision: $p=52,E=11$.</li>
                                            <li>For normal floats, $-1023 < E < 1024$, so only $2046/2048$ values are
                                                    used. The $2$ encodings encode <i>subnormal numbers</i> and $\pm
                                                    \infty$/NaN.</li>
                                            <li>Subnormal numbers: $(-1)^{s}\times(0.b_{1}b_{2}\dots b_{p})_{2} \times
                                                2^{-2022}$.</li>
                                            <li>Convention: $\operatorname{fl}(x)$ is the correctly-rounded
                                                floating-point representation of $x$, i.e. the closest FPN to $x$ with
                                                ties broken to the number with $p ^{\text{th}}$-bit $0$.</li>
                                        </ul>

                                        <p>
                                            <i>Fundamental Axiom of Floating Point Arithmetic.</i> If $x,y$ are floating
                                            point numbers,
                                        </p>

                                        $$
                                        \operatorname{fl}(x \odot y) = (x \odot y)(1 + \delta),
                                        \quad |\delta| < \varepsilon_{\text{mach}} $$ <p>
                                            where $\varepsilon_{\text{mach}} = 2^{-53} \approx 1.1\times 10^{-16}$ is
                                            the machine error.
                                </p>

                                <br>
                                <header>
                                    <h4>Cancellation Error</h4>
                                </header>

                                <p>
                                    <i>Cancellation Error.</i> If $\hat{x}=1+\delta_{1}$ and $\hat{y} =1+\delta_{2}$
                                    then possibly $\hat{x} - \hat{y} \not\approx x - y$ due to cancellation.
                                </p>

                                <div class="callout callout-example">
                                    <div class="callout-title">Cancellation in Quadratic Formula</div>
                                    <div class="callout-content">
                                        Consider the problem of finding the smaller root in the quadratic formula, i.e.
                                        $x=1-\sqrt{ 1-z }$ for $z$ small. There is large cancellation since $\sqrt{ 1-z
                                        } \approx 1$. However we can reformulate

                                        $$
                                        1 - \sqrt{ 1-z } = (1 - \sqrt{ 1-z }) \left( \frac{1+\sqrt{ 1-z }}{1 + \sqrt{
                                        1-z }} \right) = \frac{z}{1+\sqrt{ 1-z }}
                                        $$

                                        which is approximately $z /2$, even with cancellation in the denominator.
                                    </div>
                                </div>

                                <div class="callout callout-example">
                                    <div class="callout-title">Cancellation Error in Finite-Difference Approximations
                                    </div>
                                    <div class="callout-content">
                                        From second-order Taylor expansion, we can find that if $|f''|\leq C$, then

                                        $$
                                        \frac{f(x+h) - f(x)}{h} = \frac{f(x) - f(x-h)}{h} = f'(x) + \mathcal{O}(Ch)
                                        $$

                                        But taking a third-order Taylor expansion around $x\pm h$ yields

                                        $$
                                        \frac{f(x+h)-f(x-h)}{2h} = f'(x) + \mathcal{O}(Ch^{2})
                                        $$

                                        which is more accurate.
                                    </div>
                                </div>

                                <br>
                                <header>
                                    <h3>Solving Linear Systems</h3>
                                </header>

                                <br>
                                <header>
                                    <h4>Low-Rank Approximations</h4>
                                </header>

                                <p>
                                    <b>Theorem (Eckart-Young-Minsky).</b> If $A_{t}$ is the $t$-truncated SVD of $A$,
                                    then $A_{t}$ is the best rank-$t$ approximation of $A$ in the operator 2-norm and
                                    Frobenius norm:
                                </p>

                                $$
                                \begin{align}
                                \|A - A_{t}\|_{2} &= \min_{\operatorname{rank}(B)=t} \|A - B\|_{2} = \sigma_{t+1} \\
                                \|A - A_{t}\|_{F} &= \min_{\operatorname{rank}(B)=t} \|A-B\|_{F} = \sqrt{
                                \sum_{i=t+1}^{r} \sigma_{i}^{2} }
                                \end{align}
                                $$

                                <p>
                                    where $r=\operatorname{rank}(A)$.
                                </p>

                                <p>
                                    <i>Lemma (Truncated SVD).</i> The $t$-truncated SVD for $A \in \mathbb{R}^{m\times
                                    n}$ requires $\mathcal{O}(t(n+m))$ space to store. Furthermore, it takes
                                    $\mathcal{O}(mn)$ for matrix-vector multiplication, since
                                </p>

                                $$
                                A_{t}x = \left( \sum_{i=1}^{t} \sigma_{i} u_{i} v_{i}^{\top} \right) x
                                = \sum_{i=1}^{t} \sigma_{i} u_{i} (v_{i}^{\top} x)
                                $$

                                <br>
                                <header>
                                    <h4>Condition Number</h4>
                                </header>

                                <p>
                                    <i>Condition Number.</i> If $A\in \mathbb{R}^{n\times n}$ is invertible, then
                                    $\kappa_{2}(A)=\|A\|_{2}\|A^{-1}\|_{2}= \sigma_{1} /\sigma_{r}$. If $A$ is not
                                    invertible, then one can define $\kappa_{2}^{+}(A)=\|A\|\|A^{+}\|=\sigma_{1}
                                    /\sigma_{r}$.
                                </p>

                                <div class="callout callout-note">
                                    <div class="callout-title">Intuition: Condition Number</div>
                                    <div class="callout-content">
                                        Both $\kappa_{2}$ and $\kappa_{2}^{+}$ give bounds on the amplification of
                                        relative error. The motivation is that we want to measure the relative error in
                                        $y=Ax$ derived from matrix error, $\hat{A}=A+\delta A$. Then

                                        $$
                                        \frac{\|y - \hat{y}\|}{\|y\|}
                                        = \frac{\|(\delta A)x\|}{\|y\|}
                                        = \frac{\|(\delta A)A^{-1}y\|}{\|y\|}
                                        \leq \frac{\|\delta A\|}{\|A\|} \|A\| \|A^{-1}\|
                                        $$
                                        There are similar bounds if we also consider error in $y$ derived from errors in
                                        $A$ and $x$ together, or even error in $x$ from $A$ and $y$.
                                    </div>
                                </div>

                                <br>
                                <header>
                                    <h4>Back-substitution</h4>
                                </header>

                                <p>
                                    <i>Back-substitution.</i> Solve a triangular system.
                                </p>

                                <pre><code class="language-python">def solve_triangular(R, b, lower=False):
    """
    Solves triangular system Tx = b. 
    Applies backsubstitution. 
    
    Arguments: 
        A: ndarray of shape (n,n)
        b: ndarray of shape (n,)
    """
    n = b.size
    x = np.zeros_like(b)
    
    if lower:
        R = np.array(R.T, dtype=float, copy=True)
    else: 
        R = np.array(R, dtype=float, copy=True)

    if A[n-1, n-1] == 0:
        raise ValueError

    for i in range(n-1, 0, -1):
        x[i] = A[i, i]/b[i]
        for j in range (i-1, 0, -1):
            A[i, i] += A[j, i]*x[i]

    return x</code></pre>

                                <br>
                                <br>
                                <header>
                                    <h4>QR Decomposition</h4>
                                </header>

                                <p>
                                    <i>QR Decomposition.</i> An algorithm which solves $Ax=b$ by decomposing $A=QR$ for
                                    $Q \in \mathbb{R}^{m\times m}$ orthogonal and $R\in \mathbb{R}^{m\times n}$ upper
                                    triangular, and solving $Rx=Q^{\top}b$ using back-substitution. Time complexity is
                                    $\mathcal{O}(mn ^{2})$ for factorization and $\mathcal{O}(mn)$ for
                                    back-substitution.
                                </p>

                                <p>
                                    <i>Reduced/Thin/Economy QR.</i> If $m\gg n$, it is more cost-efficient to form
                                </p>

                                $$
                                A = \begin{bmatrix}
                                \hat{Q} & Q_{0}
                                \end{bmatrix}
                                \begin{bmatrix}
                                \hat{R} \\
                                0
                                \end{bmatrix}
                                = \hat{Q}\hat{R}
                                $$

                                <p>
                                    where $\hat{Q}\in \mathbb{R}^{m\times n}$ has orthonormal columns and $R\in
                                    \mathbb{R}^{n\times n}$ is upper triangular.
                                </p>

                                <div class="callout callout-question">
                                    <div class="callout-title">To which matrices should you apply QR by Householder?
                                    </div>
                                    <div class="callout-content">
                                        Matrices $A \in \mathbb{R}^{m\times n}$ with full column rank, i.e. $m \geq n$
                                        and linearly independent columns. However, heuristics exist for $m < n$ and
                                            rank-deficient cases (see `python` code). </div>
                                    </div>

                                    <p>
                                        If $A \in \mathbb{R}^{m\times n}$ has full column rank, then $R$ has full column
                                        rank and $\hat{R}$ is invertible. If $m=n$ then $R$ is invertible itself.
                                    </p>

                                    <div class="callout callout-abstract">
                                        <div class="callout-title">Reduced QR by Householder</div>
                                        <div class="callout-content">
                                            The basic idea is to find $Q_{k}$ with orthonormal columns such that
                                            $Q_{n}\dots Q_{1}A=R$, and thus $A=Q_{1}^{\top}\dots Q_{n}^{\top}R$. It
                                            proceeds as follows:
                                            <ol>
                                                <li>Let $A_{0}=A$. We will inductively preserve the following structure
                                                    in $A_{k}$:
                                                    $$
                                                    A_{k}
                                                    =
                                                    \begin{bmatrix}
                                                    R_{k} & A_{:,1: k} \\
                                                    0 & A_{:, k+1:m}
                                                    \end{bmatrix}
                                                    \equiv
                                                    \begin{bmatrix}
                                                    R_{k} & B_{1} \\
                                                    0 & B_{2}
                                                    \end{bmatrix}
                                                    $$
                                                </li>
                                                <li>Take $A_{k-1}$. We need $Q_{k-1}$ with orthonormal columns so that
                                                    $A_{k}:=Q_{k}A_{k-1}$ satisfies
                                                    $$
                                                    A_{k} = Q_{k-1} A_{k-1}
                                                    = \begin{bmatrix}
                                                    I_{k-1} & 0 \\
                                                    0 & H
                                                    \end{bmatrix}
                                                    \begin{bmatrix}
                                                    R_{k-1} & B_{1} \\
                                                    0 & B_{2}
                                                    \end{bmatrix}
                                                    =
                                                    \begin{bmatrix}
                                                    R_{k-1} & 0 \\
                                                    0 & HB_{2}
                                                    \end{bmatrix}
                                                    $$
                                                </li>
                                                <li>Let $y=A_{k,k:m}$. We need $Hy \propto e_{1}$ to "zero-out" the last
                                                    $m-k$ entries of the $k^{\text{th}}$ column. $H$ satisfies
                                                    $H^{\top}H=I$, so $\|Hy\|=\|y\|\implies Hy =\pm\|y\|e_{1}$. If we
                                                    pick $v=\|y\|e_{1}-y$ and $u=v /\|v\|$, then we must have $H=I-2u
                                                    u^{\top}$ to send $Hy=\|y\|e_{1}$.</li>
                                                <li>To avoid cancellation error when computing $v$, choose
                                                    $v=-\operatorname{sgn}(y)\|y\|e_{1}-y$ which is equivalent to $v =
                                                    \operatorname{sgn}(y)\|y\|e_{1}+y$ since the signs cancel in
                                                    $vv^{\top}$. It can easily be verified each constructed $H$ actually
                                                    satisfies $H^{\top}H=I$.</li>
                                                <li>To solve $Ax=b$, store the Householder vectors $\{ u_{k} \in
                                                    \mathbb{R}^{m-k+1} \}_{k}$. Then, solve $Rx=Q_{n}\dots Q_{1}b$ via
                                                    $b_{k:m}\leftarrow b_{k:m}-2u_{k}u_{k}^{\top}b_{k:m}$. Conclude with
                                                    back-substitution.</li>
                                            </ol>
                                        </div>
                                    </div>

                                    <p>
                                        <i>Partial Pivoting.</i> To improve numerical stability, before forming each
                                        $A_{k}$ we permute the remaining $n-k+1$ columns to select the largest $\ell
                                        ^{2}$ norm. We do this by initializing $c_{l}=\|A_{:,l}\|^{2}$ and then updating
                                        $c_{l}\leftarrow c_{l}-A_{k,l}^{2}$ after applying the Householder reflector. At
                                        the end, we obtain $A\Pi=Q^{\top}R$ from which we can invert the permutation
                                        matrix.
                                    </p>

                                    <pre><code class="language-python">import numpy as np
from scipy.linalg import solve_triangular

def solve_qr(A, b, tol=None):
    """
    Solve Ax = b using Householder QR with column pivoting.
    If A is not full rank, uses rank heuristic. 

    Arguments:
        A: ndarray of shape (m, n)
        b: ndarray of shape (m,)

    Returns:
        If m >= n: least-squares solution
        If m <  n: minimum-norm solution
    """
    A = np.array(A, dtype=float, copy=True)
    b = np.array(b, dtype=float, copy=True)
    m, n = A.shape
    kmax = min(m, n)

    piv = np.arange(n)
    col2 = np.sum(A*A, axis=0).astype(float)

    reflectors = []

    for k in range(kmax):
        # pivot max-norm column in A[k:, k:]
        j = k + np.argmax(col2[k:])
        if j != k:
            A[:, [k, j]] = A[:, [j, k]]
            col2[[k, j]] = col2[[j, k]]
            piv[[k, j]] = piv[[j, k]]

        # construct Householder reflector for column k
        y = A[k:, k]
        y_norm = np.linalg.norm(y)
        if y_norm == 0.0:
            v = np.zeros_like(y)
            beta = 0.0
        else:
            alpha = -np.copysign(y_norm, y[0])
            v = y.copy()
            v[0] -= alpha
            v_norm = np.linalg.norm(v)
            if v_norm == 0.0:
                v[:] = 0.0
                beta = 0.0
            else:
                v /= v_norm
                beta = 2.0

        reflectors.append((k, v, beta))

        # apply reflector to trailing block
        if beta != 0.0:
            A[k:, k:] -= beta * np.outer(v, v @ A[k:, k:])

        # cheap column-norm updates for selection
        if k + 1 < n:
            col2[k+1:] -= A[k, k+1:]**2
            col2[k+1:] = np.maximum(col2[k+1:], 0.0)

    y = b.copy()
    for (k, v, beta) in reflectors:
        if beta != 0.0:
            y[k:] -= beta * v * (v @ y[k:])

    R = np.triu(A[:kmax, :n])

    # rank heuristic
    if kmax > 0:
        diag = np.abs(np.diag(R[:kmax, :kmax]))
        if tol is None:
            eps = np.finfo(float).eps
            scale = diag.max() if diag.size else 1.0
            tol = max(m, n) * scale * eps
        r = int(np.sum(diag > tol))
    else:
        r = 0

    x = np.zeros(n)

    if m >= n:
        z = np.zeros(n)
        if r > 0:
            z[:r] = solve_triangular(R[:r, :r], y[:r], lower=False)
        x[piv] = z

    else:
        z = np.zeros(n)
        if r > 0:
            z[:r] = solve_triangular(R[:r, :r], y[:r], lower=False)
        x[piv] = z

    return x</code></pre>

                                    <div class="callout callout-question">
                                        <div class="callout-title">What is the time complexity of QR by Householder?
                                        </div>
                                        <div class="callout-content">
                                            Asymptotically $\mathcal{O}(mn ^{2})$, since updating $A_{k}$ takes
                                            $\mathcal{O}(mn)$ due to vector-matrix multiplication, and there are $n$
                                            updates.
                                        </div>
                                    </div>

                                    <p>
                                        <i>Lemma (Stability of QR by Householder).</i> There exists $\delta A$ such that
                                        $\hat{Q}\hat{R}=A+\delta A$ and $\|\delta A\|
                                        /\|A\|=\mathcal{O}(\varepsilon_{\text{mach}})$. In other words, the algorithm is
                                        backwards stable.
                                    </p>

                                    <p>
                                        <i>Lemma (Orthogonality of $Q$).</i> In QR by Householder,
                                        $\|Q^{\top}Q-I\|=\mathcal{O}(\varepsilon_{\text{mach}})$.
                                    </p>

                                    <br>
                                    <header>
                                        <h4>LU Decomposition</h4>
                                    </header>

                                    <p>
                                        <i>LU Decomposition.</i> An algorithm which solves $Ax=b$ for square $A \in
                                        \mathbb{R}^{n\times n}$ by factoring $A=LU$ where $L,U\in \mathbb{R}^{n\times
                                        n}$ are lower and upper triangular, respectively.
                                    </p>

                                    <div class="callout callout-abstract">
                                        <div class="callout-title">LU by Gauss Transformations</div>
                                        <div class="callout-content">
                                            The idea is to form $L_{n-1}\dots L_{1}A=U$ and solve $x=L_{1}^{-1}\dots
                                            L_{n-1}^{-1}Ub$ by back-substitution.
                                            <ol>
                                                <li>Let $A_{0}=A$. To form $A_{k}$ for $k=1,\dots,n$ we want to apply
                                                    $L_{k-1}A_{k-1}$ to "zero-out" the bottom $\texttt{k+1:n}$ entries.
                                                    In other words, the $k ^{\text{th}}$ column $x_{k}$ of $A_{k-1}$
                                                    satisfies
                                                    $$
                                                    x_{k} =
                                                    \begin{bmatrix}
                                                    a_{1,k} \\
                                                    \vdots \\
                                                    a_{k,k} \\
                                                    a_{k+1,k} \\
                                                    \vdots \\
                                                    a_{n,k}
                                                    \end{bmatrix}
                                                    \stackrel{L_{k}}{\longrightarrow}
                                                    \begin{bmatrix}
                                                    a_{1,k} \\
                                                    \vdots \\
                                                    a_{k,k} \\
                                                    0 \\
                                                    \vdots \\
                                                    0
                                                    \end{bmatrix}
                                                    \implies
                                                    L_{k} = \begin{bmatrix}
                                                    1 & & & & & \\
                                                    & \ddots & & & & \\
                                                    & & & 1 & & & \\
                                                    & & & -l_{k+1,k} & \ddots & & \\
                                                    & & & \vdots & & \ddots & \\
                                                    & & & -l_{n,k} & & & 1
                                                    \end{bmatrix}
                                                    = I - l_{k} e_{k}^{\top}
                                                    $$
                                                    where $l_{j,k}=a_{j,k} /a_{k,k}$ for all $k < j \leq n$ and $0$
                                                        elsewhere. </li>
                                                <li>Each $L_{k}$ is a <i>Gauss transformation</i>, and an important
                                                    observation is that $L_{k}^{-1}=I-l_{k}e_{k}^{\top}$, while
                                                    $L_{1}^{-1}\dots
                                                    L_{n-1}^{-1}=I+l_{1}e_{1}^{\top}+\dots+l_{n-1}e_{n-1}^{\top}$. (The
                                                    idea is $l_{j}e_{k}^{\top}=0$ whenever $j < k$.)</li>
                                                <li>Finally we evaluate $y=Ub$ and then
                                                    $y+l_{1}y_{1}+l_{2}y_{2}+\dots+l_{n-1}y_{n-1}$.</li>
                                            </ol>
                                        </div>
                                    </div>

                                    <div class="callout callout-question">
                                        <div class="callout-title">Why are there $n-1$ matrices in LU but $n$ in QR?
                                        </div>
                                        <div class="callout-content">
                                            LU applies only to square matrices, so there are no entries below the $n
                                            ^{\text{th}}$ row of the $n ^{\text{th}}$ column. On the other hand, since
                                            $m \geq n$ generally in QR, we will need to zero entries explicitly even in
                                            the $n ^{\text{th}}$ column.
                                        </div>
                                    </div>

                                    <p>
                                        <i>Growth Factor.</i> If the pivots for LU are very small, then numerical
                                        instability may occur since we divide by a tiny number to compute $l_{k}$. The
                                        growth factor measures this blowup,
                                    </p>

                                    $$
                                    G(A) = \frac{\|L\|_{\infty} \|U\|_{\infty}}{\|A\|_{\infty}}
                                    $$

                                    <p>
                                        where $\|X\|_{\infty}$ is the max-norm on the vectorization of $X$. If $G(A)$ is
                                        not too large then LU is backwards stable.
                                    </p>

                                    <p>
                                        <b>Wilkinson's Theorem.</b> For some $P,Q$ permutation matrices,
                                        $G(PAQ)=\mathcal{O}(n ^{1/2+1/4\log n})$.
                                    </p>

                                    <div class="callout callout-note">
                                        <div class="callout-title">Partial Pivoting</div>
                                        <div class="callout-content">
                                            In practice we only use partial pivoting, i.e. $PA=LU$. Partial pivots
                                            require $\mathcal{O}(n ^{2})$ which is negligible relative to the LU cost
                                            itself of $\mathcal{O}(n ^{3})$, but complete pivoting requires
                                            $\mathcal{O}(n ^{3})$.
                                        </div>
                                    </div>

                                    <div class="callout callout-abstract">
                                        <div class="callout-title">Partial Pivoting by Max Entry</div>
                                        <div class="callout-content">
                                            The most popular technique. For each $k$,
                                            <ol>
                                                <li>Find $i=\arg\max_{i \geq k}|a_{i,k}|$.</li>
                                                <li>Swap rows $i$ and $k$ for the current matrix.</li>
                                                <li>Apply the Gauss transformation. The invariant is
                                                    $L_{n-1}P_{n-1}\dots L_{1}P_{1}A=U$.</li>
                                            </ol>
                                            The key idea is that $PLP ^{-1}=PLP$ is still a Gauss transformation if $L$
                                            is one.
                                        </div>
                                    </div>

                                    <pre><code class="language-python">import numpy as np
from scipy.linalg import solve_triangular

def solve_lu(A, b, tol=1e-12):
    A = np.array(A, dtype=float)
    b = np.array(b, dtype=float)
    m, n = A.shape
    if m != n: 
        raise ValueError("A must be square")

    for k in range(n-1): 
        j = k + np.argmax(np.abs(A[k:, k]))
        if abs(A[j, k]) < tol:
            raise ValueError("A is singular")
        if j != k: 
            A[[k, j]] = A[[j, k]]
            b[[k, j]] = b[[j, k]]

        for i in range(k+1, n): 
            l = A[i, k] / A[k, k]
            A[i, k:] -= l * A[k, k:]
            b[i] -= l * b[k]

    if abs(A[n-1, n-1]) < tol: 
        raise ValueError("A is singular or ill-conditioned")

    x = solve_triangular(A, b, lower=False) 
    return x</code></pre>

                                    <div class="callout callout-question">
                                        <div class="callout-title">What is the complexity of LU?</div>
                                        <div class="callout-content">
                                            The factorization requires $\mathcal{O}(n ^{3})$ time, which is around twice
                                            as fast (due to better constant factors) than QR. In fact `np.linalg.solve`
                                            implements LU with partial pivoting.
                                        </div>
                                    </div>

                                    <div class="callout callout-abstract">
                                        <div class="callout-title">Block LU</div>
                                        <div class="callout-content">
                                            Suppose we're given an invertible block matrix $A$ and seek its LU
                                            factorization,

                                            $$
                                            A = \begin{bmatrix}
                                            A_{11} & A_{12} \\
                                            A_{21} & A_{22}
                                            \end{bmatrix}
                                            = \begin{bmatrix}
                                            L_{11} & 0 \\
                                            L_{21} & L_{22}
                                            \end{bmatrix}
                                            \begin{bmatrix}
                                            U_{11} & U_{12} \\
                                            0 & U_{22}
                                            \end{bmatrix}
                                            $$

                                            Here $A_{11} \in \mathbb{R}^{n\times n}$ and $A_{22} \in \mathbb{R}^{p\times
                                            p}$ but note $A_{21}$ and $A_{12}$ need not be square. If $A_{11}$ is
                                            invertible,
                                            <ol>
                                                <li>Factorize $A_{11}=L_{11}U_{11}$. Complexity $\mathcal{O}(n ^{3})$.
                                                </li>
                                                <li>Solve $L_{11}U_{12}=A_{12}$ by Gaussian elimination for $U_{12}$.
                                                    Complexity $\mathcal{O}(n ^{2})$.</li>
                                                <li>Solve $L_{21}U_{11}=A_{21}$ by Gaussian elimination for $L_{21}$.
                                                    Complexity $\mathcal{O}(n ^{2})$.</li>
                                                <li>Factor $S=A_{22}-L_{21}U_{12}$ to obtain $L_{22}$ and $U_{22}$.
                                                    Complexity $\mathcal{O}(p ^{3})$.</li>
                                            </ol>
                                            The benefits of Block LU are efficient factorization updates (for instance,
                                            if only $A_{22}$ changes) and parallel computation of independent blocks.
                                            The matrix $S$ is the <i>Schur complement.</i>
                                        </div>
                                    </div>

                                    <br>
                                    <header>
                                        <h4>Cholesky Decomposition</h4>
                                    </header>

                                    <p>
                                        <b>Theorem.</b> Every SPD matrix has a unique Cholesky decomposition
                                        $A=R^{\top}R$ for $R$ upper triangular with strictly positive diagonal entries,
                                        and $\|R\|^{2}=\|A\|$.
                                    </p>

                                    <br>
                                    <header>
                                        <h3>Optimization</h3>
                                    </header>

                                    <br>
                                    <header>
                                        <h4>Unconstrained Optimization</h4>
                                    </header>

                                    <p>
                                        <i>Lemma (Optimality Conditions).</i> Let $f:\mathbb{R}^{n}\to \mathbb{R}$ be a
                                        function.
                                    </p>
                                    <ul>
                                        <li>First-Order Test: $\nabla f(x^{*})=0$ for a local minimum (necessary).</li>
                                        <li>Second-Order Test: $\nabla ^{2} f(x^{*})$ is symmetric PSD for a local
                                            minimum (necessary, sufficient). If additionally $\nabla ^{2}f(x)$ is PSD
                                            for all $x$, then the minimum is unique and global, and in fact $f$ is
                                            strictly convex.</li>
                                    </ul>

                                    <div class="callout callout-abstract">
                                        <div class="callout-title">Quasi-Newton Methods in High-Dimensions</div>
                                        <div class="callout-content">
                                            From the Taylor expansion, we can find roots to $f:\mathbb{R}^{n}\to
                                            \mathbb{R}$ under appropriate smoothness conditions. For $f\in C^{3}$,

                                            $$
                                            f(x+p) \approx f(x) + \nabla f(x)^{\top}p + \frac{1}{2}p^{\top}\nabla
                                            ^{2}f(x)p
                                            $$

                                            This second-order approximation vanishes when

                                            $$
                                            \nabla_{p}f(x+p)
                                            = \nabla f(x) + \nabla ^{2}f(x) p
                                            = 0
                                            \implies
                                            p = -(\nabla ^{2}f(x))^{-1} \nabla f(x)
                                            $$

                                            Quasi-Newton iterates thus take the form

                                            $$
                                            x_{k+1} = x_{k} - \alpha_{k} B_{k}^{-1} \nabla f(x_{k})
                                            \equiv x_{k} + \alpha_{k} p_{k}
                                            $$

                                            where $B_{k}\to \nabla ^{2}f(x_{k})$ in norm, $p_{k}=-B_{k}^{-1}\nabla
                                            f(x_{k})$ is the step, and $B_{k}$ is SPD so that $p_{k}$ is a descent
                                            direction for properly chosen $\alpha_{k}$. An appropriate step size
                                            $\alpha_{k}$ must exist if small enough, so a common approach is
                                            <i>backtracking line search</i>: halve an initial guess until a stopping
                                            condition is met.
                                        </div>
                                    </div>

                                    <p>
                                        <i>Armijo Condition.</i> The stopping condition satisfies
                                        $f(x_{k}+\alpha_{k}p_{k})\leq f(x_{k})+c \alpha_{k}\nabla f(x_{k})^{\top}p_{k}$
                                        where $c\in(0,1)$ is constant. The idea is to always ensure at least a constant
                                        fraction of the linear prediction term for a decrease, preventing arbitrary
                                        small perturbations.
                                    </p>

                                    <p>
                                        <i>Gradient Descent.</i> When $B_{k}=I$, we have gradient descent. Since $I$
                                        does not tend to $\nabla ^{2}f$, descent is not guaranteed. For convex
                                        functions, convergence is sublinear, $\|e_{k}\|=\mathcal{O}(1 /k)$. The
                                        advantage is that the per-iteration update is cheap, requiring only
                                        $\mathcal{O}(n)$ to compute gradients.
                                    </p>

                                    $$
                                    x_{k+1} = x_{k} - \alpha_{k} \nabla f(x_{k})
                                    $$

                                    <p>
                                        <i>Newton's Method.</i> When $B_{k}=\nabla ^{2}f(x_{k})$, we have Newton's
                                        method. If $\nabla ^{2}f$ satisfies the second-order test and is locally
                                        Lipschitz, then quadratic convergence is guaranteed. The per-iteration cost is
                                        impractical, though, at $\mathcal{O}(n ^{3})$ per iteration.
                                    </p>

                                    $$
                                    x_{k+1} = x_{k} - \alpha_{k} \nabla ^{2}f(x_{k})^{-1} \nabla f(x_{k})
                                    $$

                                    <p>
                                        <b>Theorem (Superlinear Convergence).</b> If $f \in C^{3}(\mathbb{R}^{n} \to
                                        \mathbb{R})$, $\|(\nabla ^{2}f)^{-1}\|\leq B$ locally around $x^{*}$, and $B_{k}
                                        \to \nabla ^{2}f(x_{k})$, then $x_{k} \to x^{*}$ superlinearly.
                                    </p>

                                    <div class="callout callout-question">
                                        <div class="callout-title">What are conditions for choosing $B_{k}$?</div>
                                        <div class="callout-content">
                                            To ensure each iterate has a descent direction, we want $B_{k}$ (thus
                                            $B_{k}^{-1}$) to be SPD:

                                            $$
                                            f(x + \alpha p) \approx f(x) + \alpha \nabla f(x)^{\top} p
                                            = f(x) - \alpha \nabla f(x)^{\top} B_{k}^{-1} \nabla f(x) < f(x) $$ for
                                                $\alpha$ small enough. Furthermore if $B_{k}$ approximates the Hessian
                                                then $$ \nabla f(x_{k+1}) \approx \nabla f(x_{k}) + \nabla
                                                ^{2}f(x_{k})(x_{k+1} - x_{k}) \approx \nabla f(x_{k}) + B_{k} (x_{k+1} -
                                                x_{k}) $$ and hence $B_{k}s_{k}=y_{k}$, where $s_{k}=x_{k+1}-x_{k}$
                                                while $y_{k}=\nabla f(x_{k+1}) - \nabla f(x_{k})$. However, even with
                                                these $n$ constraints, any SPD $B_{k}$ still has $n(n+1) /2$ degrees of
                                                freedom to be chosen. </div>
                                        </div>

                                        <p>
                                            <i>SR1.</i> A cheap update is $B_{k+1}=B_{k}+\beta_{k}v_{k}v_{k}^{\top}$ for
                                            $v_{k}=y_{k}-B_{k}s_{k}$ and $\beta_{k}=1 /v_{k}^{\top}s_{k}$. With some
                                            regulatory conditions it converges superlinearly, but $\beta_{k}$ might
                                            become negative, causing $B_{k}$ to no longer remain SPD. By
                                            Sherman-Morrison, the inverses can be quickly updated, however.
                                        </p>

                                        <p>
                                            <i>BFGS.</i> If $B_{k}$ is SPD then the following update preserves this
                                            property:
                                        </p>

                                        $$
                                        B_{k+1} = B_{k} - \frac{B_{k}s_{k}s_{k}^{\top}B_{k}}{s_{k}^{\top}B_{k}s_{k}} +
                                        \frac{y_{k}y_{k}^{\top}}{y_{k}^{\top}s_{k}}
                                        \equiv B_{k} - \frac{B_{k}s_{k}s_{k}^{\top}B_{k}}{s_{k}^{\top}B_{k}s_{k}} +
                                        \rho_{k} y_{k}y_{k}^{\top}
                                        $$

                                        <p>
                                            And the inverses are updated as (assume $H_{0}=I$)
                                        </p>

                                        $$
                                        \begin{aligned}
                                        H_{k+1} &= (I - \rho_{k} s_{k}y_{k}^{\top}) H_{k} (I - \rho_{k}
                                        y_{k}s_{k}^{\top}) + \rho_{k} s_{k}s_{k}^{\top} \\
                                        &= (I-\rho_{k} s_{k}y_{k}^{\top})\cdots(I-\rho_{1} s_{1}y_{1}^{\top})(I-\rho_{1}
                                        y_{1}s_{1})\cdots(I-\rho_{k} y_{k}s_{k}^{\top}) + \sum_{i=1}^{k} \rho_{i}
                                        s_{i}s_{i}^{\top} \\
                                        \end{aligned}
                                        $$

                                        <p>
                                            A further improvement known as <i>L-BFGS</i> only takes the last $m$ pairs,
                                        </p>

                                        $$
                                        H_{k+1} = \prod_{i=1}^{m} (I - \rho_{k-i+1} s_{k-i+1} y_{k-i+1}^{\top}) \cdot
                                        \prod_{i=1}^{m} (I - \rho_{i}y_{i}s_{i}^{\top}) + \sum_{i=1}^{k} \rho_{i} s_{i}
                                        s_{i}^{\top}
                                        $$

                                        <p>
                                            It may be the most widely-used algorithm for unconstrained optimization.
                                        </p>

                                        $$
                                        \begin{align}
                                        & \mathrm{Initialize}\; B_{0} \leftarrow I,\; x \leftarrow x_{0} \\
                                        & \mathbf{for}\; k=0,1,2,\dots \; \mathbf{do} \\
                                        & \qquad \mathrm{Solve}\; p_{k} \leftarrow H_{k} \nabla f(x_{k}) \text{
                                        iteratively} \\
                                        & \qquad \mathrm{Find}\; \alpha_{k} \; \text{using Armijo backtracking line
                                        search} \\
                                        & \qquad \mathrm{Set}\; x_{k+1} \leftarrow x_{k} + \alpha_{k} p_{k} \\
                                        & \qquad \mathrm{Compute}\; s_{k+1} \leftarrow x_{k+1} - x_{k},\; y_{k+1}
                                        \leftarrow \nabla f(x_{k+1}) - \nabla f(x_{k}) \\
                                        & \qquad \mathrm{Update}\; B_{k+1} \;\text{by the BFGS rank-2 update} \\
                                        & \mathbf{end\; for}
                                        \end{align}
                                        $$

                                        <p>
                                            The parameter $m$ is usually chosen to be small (i.e. $m \leq 20$) so
                                            per-iteration is $\mathcal{O}(n)$. If the derivatives are not provided then
                                            finite-difference approximations can estimate them.
                                        </p>

                                        <br>
                                        <header>
                                            <h3>Eigenstuff Computation</h3>
                                        </header>

                                        <br>
                                        <header>
                                            <h4>Eigenvalues</h4>
                                        </header>

                                        <br>
                                        <header>
                                            <h5>Schur Decomposition</h5>
                                        </header>

                                        <div class="callout callout-question">
                                            <div class="callout-title">How do you find eigenvalues of $A \in
                                                \mathbb{C}^{n \times n}$?</div>
                                            <div class="callout-content">
                                                The key idea is to compute the Schur factorization via

                                                $$
                                                Q_{j}^{\dagger} \dots Q_{1}^{\dagger} A Q_{1}\dots Q_{j} \stackrel{j \to
                                                \infty}{\longrightarrow} T
                                                $$

                                                where each $Q_{j}$ is unitary. Practical algorithms split this process
                                                into two steps:
                                                <ol>
                                                    <li>Reduction to Hessenberg. Apply unitary transformations to get
                                                        $H$ which is diagonal plus one subdiagonal. Costs $\mathcal{O}(n
                                                        ^{3})$, about 2x as much as Householder.</li>
                                                    <li>QR Algorithm. Reduce $H$ to upper triangular. If $A$ is
                                                        symmetric, $H$ is tridiagonal, and the convergence is very
                                                        quick.</li>
                                                </ol>
                                            </div>
                                        </div>

                                        <div class="callout callout-note">
                                            <div class="callout-title">Existence of Hessenberg Decomposition</div>
                                            <div class="callout-content">
                                                Every $A \in \mathbb{R}^{n\times n}$ admits a Hessenberg $H$ and an
                                                orthogonal $Q$ such that $H=Q^{\top}AQ$.
                                            </div>
                                        </div>

                                        <p>
                                            <b>Phase 1.</b> Find $Q_{1},\dots,Q_{n-2}$ orthogonal such that $Q_{j}$
                                            zeros out the necessary elements of column $j$. In particular,
                                        </p>

                                        $$
                                        Q_{j} = \begin{bmatrix}
                                        I_{j} & 0 \\
                                        0 & F_{n-j}
                                        \end{bmatrix}
                                        \implies Q_{1}\dots Q_{n-2} A Q_{1}\dots Q_{n-2} = H
                                        $$

                                        <p>
                                            where $F_{j}$ is the same as in Householder. Note $Q_{j}$ is symmetric.
                                        </p>

                                        <p>
                                            <i>Lemma.</i> If $A$ is symmetric then $H=Q^{\top}AQ$ is symmetric as well
                                            and therefore must be tridiagonal, such as
                                        </p>

                                        $$
                                        H = \begin{bmatrix}
                                        \times & \times & 0 & \cdots & 0 \\
                                        \times & \times & \times & \ddots & \vdots \\
                                        0 & \times & \times & \ddots & 0 \\
                                        \vdots & \ddots & \ddots & \ddots & \times \\
                                        0 & \cdots & 0 & \times & \times
                                        \end{bmatrix}
                                        $$

                                        <p>
                                            <b>Phase 2.</b> Find a QR factorization $H=\hat{Q}T$. Then
                                            $T=\hat{Q}^{\top}H$ has diagonal elements equal to the eigenvalues of $A$.
                                        </p>

                                        <div class="callout callout-question">
                                            <div class="callout-title">What are the time complexities?</div>
                                            <div class="callout-content">
                                                <ol>
                                                    <li>Phase 1: Costs $\mathcal{O}(n ^{2})$ per multiplication, so
                                                        about $\mathcal{O}(n ^{3})$ with 2x leading constant factors as
                                                        QR by Householder.</li>
                                                    <li>Phase 2: Costs $\mathcal{O}(n ^{2})$ to factorize a Hessenberg
                                                        matrix, then $\mathcal{O}(n ^{2})$ for matrix multiplication
                                                        thanks to Hessenberg property and since only the diagonal
                                                        matters.</li>
                                                </ol>
                                            </div>
                                        </div>

                                        <br>
                                        <header>
                                            <h5>Shifted QR Algorithm</h5>
                                        </header>

                                        <p>
                                            <b>Theorem (Convergence).</b> Under appropriate conditions $A^{(k)} \to T$
                                            with the eigenvalues of $A$ on the diagonal.
                                        </p>

                                        $$
                                        \begin{align}
                                        & A^{(0)} \leftarrow A \\
                                        & \mathbf{for}\; k = 1, 2, 3, \dots \; \mathbf{do} \\
                                        & \qquad \text{Compute QR factorization:}\; Q^{(k)}R^{(k)} = A^{(k-1)} \\
                                        & \qquad \text{Form new matrix:}\; A^{(k)} \leftarrow R^{(k)}Q^{(k)} \\
                                        & \mathbf{end\; for}
                                        \end{align}
                                        $$

                                        <p>
                                            The idea is once we show $V^{(k)}=A^{k}V^{(0)}$ has its first column
                                            converge to an eigenvector (true if we assume $\lambda_{1}$ is dominant),
                                            then because $Q$ is orthogonal, none of the others columns can converge to
                                            the same eigenvector, and we proceed inductively. However for numerical
                                            stability we need to normalize the columns (e.g. by computing another QR),
                                            i.e.
                                        </p>

                                        $$
                                        \begin{align}
                                        & \text{Require: Matrix } A \in \mathbb{R}^{n \times n}, \text{ initial }
                                        Q^{(1)} \in \mathbb{R}^{n \times m} \text{ ON columns} \\
                                        & \mathbf{for}\; k = 1, 2, \dots \; \mathbf{do} \\
                                        & \qquad Z \leftarrow AQ^{(k)} \\
                                        & \qquad \text{Compute QR: } Q^{(k+1)}R^{(k)} = Z \\
                                        & \mathbf{end\; for}
                                        \end{align}
                                        $$

                                        <p>
                                            Incorporate three more tricks to make it practical:
                                        </p>
                                        <ol>
                                            <li>Reduce to Hessenberg structure, which is preserved by iterations,</li>
                                            <li>Factor $A^{(k-1)}-\mu ^{(k)} I$ instead of $A^{(k-1)}$ where $\mu
                                                ^{(k)}$ is some well-chosen pivot e.g. $A^{(k-1)}_{nn}$,</li>
                                            <li>Deflate any off-diagonal entries $|a_{ij}| \leq
                                                \varepsilon_{\text{mach}}$ to $0$.</li>
                                        </ol>

                                        <p>
                                            <i>Shifted QR Algorithm.</i>
                                        </p>

                                        $$
                                        \begin{align}
                                        & \text{Reduce } A \text{ to Hessenberg/tridiagonal form: } \tilde{Q}^T A
                                        \tilde{Q} = H \\
                                        & \text{Set } A^{(0)} \leftarrow H \\
                                        & \mathbf{for}\; k = 1, 2, \dots \text{ until convergence } \mathbf{do} \\
                                        & \qquad \text{Choose shift } \mu^{(k)} \text{ (e.g., } \mu^{(k)} =
                                        A_{nn}^{(k-1)}) \\
                                        & \qquad \text{Compute QR: } Q^{(k)}R^{(k)} = A^{(k-1)} - \mu^{(k)}I \\
                                        & \qquad \text{Form } A^{(k)} = R^{(k)}Q^{(k)} + \mu^{(k)}I \\
                                        & \qquad \mathbf{if}\; \text{any off-diagonal element } |A_{j,j+1}^{(k)}| <
                                            \varepsilon_{\text{mach}} \; \mathbf{then} \\ & \qquad \qquad \text{Set }
                                            A_{j,j+1}^{(k)}=A_{j+1,j}^{(k)}=0 \text{ (deflation)} \\ & \qquad \qquad
                                            \text{Apply QR algorithm recursively to the decoupled blocks} \\ & \qquad
                                            \mathbf{end\; if} \\ & \mathbf{end\; for} \end{align} $$ <div
                                            class="callout callout-question">
                                            <div class="callout-title">What is the complexity of practical QR algorithm?
                                            </div>
                                            <div class="callout-content">
                                                <ol>
                                                    <li>Hessenberg reduction: $\mathcal{O}(n ^{3})$.</li>
                                                    <li>Per-iteration: $\mathcal{O}(n ^{2})$, or $\mathcal{O}(n)$ if
                                                        symmetric (tridiagonal).</li>
                                                    <li>Total: $\mathcal{O}(n ^{3})$ in general, dominated by Phase 2.
                                                        $\mathcal{O}(n ^{3})$ for symmetric, dominated by Phase 1, since
                                                        3-4 iterations per eigenvalue.</li>
                                                </ol>
                                            </div>
                                    </div>

                                    <br>
                                    <header>
                                        <h4>Eigenvectors</h4>
                                    </header>

                                    <br>
                                    <header>
                                        <h5>Power Iteration</h5>
                                    </header>

                                    <div class="callout callout-question">
                                        <div class="callout-title">How can we find a single (dominant) eigenvector?
                                        </div>
                                        <div class="callout-content">
                                            <i>Power iteration</i> finds a dominant (strictly largest eigenvalue
                                            magnitude) eigenvector if it exists. The iterates are $v_{k}=A^{k}v_{0}$
                                            with normalization at each step.
                                        </div>
                                    </div>

                                    <p>
                                        <b>Theorem.</b> If $|\lambda_{1}|>|\lambda_{2}|\geq\dots$ are eigenvalues, so
                                        that $\lambda_{1}$ is a dominant eigenvalue with eigenvector $q_{1}$, and
                                        $b_{1}=q_{1}^{\top}v_{0} \neq 0$ (the guess needs to be nonzero in the direction
                                        of the $q_{1}$), then power iteration converges linearly to $q_{1}$ with rate
                                        $|\lambda_{2} /\lambda_{1}|$.
                                    </p>

                                    <p>
                                        <i>Proof.</i> Expand $v_{k}$ in the eigenbasis, then factor out
                                        $\lambda_{1}^{k}$. $\blacksquare$
                                    </p>

                                    <br>
                                    <header>
                                        <h5>Inverse Iteration</h5>
                                    </header>

                                    <div class="callout callout-question">
                                        <div class="callout-title">How can we find the entire spectrum?</div>
                                        <div class="callout-content">
                                            <i>Inverse iteration</i> finds nearby eigenvalues from a guess by using the
                                            characteristic matrix and power iteration. Let $B=(A - \mu I)^{-1}$ and by
                                            substituting via Schur's $A=QTQ^{\top}$ we see that $B$ has the same
                                            eigenvalues as $(T - \mu I)^{-1}$, which are $(\lambda_{i}-\mu)^{-1}$. In
                                            other words if $\mu$ is close to $\lambda_{i}$ then it is a dominant
                                            eigenvalue of $B$!
                                        </div>
                                    </div>

                                    <p>
                                        <b>Theorem.</b> If $q_{I}$ and $q_{J}$ are the closest and second-closest
                                        eigenvalues to $\mu$, respectively, then inverse iterations (power iterations on
                                        $B$, i.e. $v_{k}=B^{k}v_{0}$) converge to $q_{I}$ linearly with rate
                                        $\frac{|\lambda_{I}-\mu|}{|\lambda_{J}-\mu|}$.
                                    </p>

                                    <br>
                                    <header>
                                        <h5>Rayleigh Iteration</h5>
                                    </header>

                                    <div class="callout callout-summary">
                                        <div class="callout-title">Motivation for the Rayleigh Quotient</div>
                                        <div class="callout-content">
                                            You are given $x$ which is close to an eigenvector. How do you find an
                                            (approximate) eigenvector $\alpha$? Formulate as optimization:
                                            $$
                                            \operatorname{minimize} \|Ax - \alpha x\|^{2}
                                            \implies x^{\top}(\alpha x-Ax) = 0
                                            \implies \alpha = \frac{x^{\top}Ax}{x^{\top}x}
                                            $$
                                            We denote $r(x)=\alpha$ to be the <i>Rayleigh quotient</i>, and if $x=q_{i}$
                                            is an eigenvector then $r(x)=\lambda_{i}$, its corresponding eigenvalue.
                                        </div>
                                    </div>

                                    <p>
                                        <b>Theorem.</b> If $\|x-q_{i}\|\leq \varepsilon$ then $|r(x)-\lambda_{i}|\leq
                                        \mathcal{O}(\varepsilon ^{2})$.
                                    </p>

                                    <p>
                                        <i>Proof.</i> It can be shown that $q_{i}$ are stationary points of $r(x)$, so
                                        by Taylor expansion,
                                    </p>

                                    $$
                                    r(x) = r(q_{i}) + \nabla r(q_{i})^{\top}(x - q_{i}) + \mathcal{O}(\|x-q_{i}\|^{2})
                                    = \lambda_{i} + 0 + \mathcal{O}(\varepsilon ^{2})
                                    $$

                                    <p>
                                        so that $|r(x) - \lambda_{i}| \leq \mathcal{O}(\varepsilon ^{2})$.
                                        $\blacksquare$
                                    </p>

                                    <div class="callout callout-note">
                                        <div class="callout-title">Summary of Current Progress</div>
                                        <div class="callout-content">
                                            <ol>
                                                <li>Inverse Iteration: given $\mu \approx \lambda_{i}$, we can find
                                                    $q_{i} \approx B^{k}v_{0}$ for large $k$.</li>
                                                <li>Rayleigh Quotient: given $x\approx q_{i}$, we can find $r(x)\approx
                                                    \lambda_{i}$.</li>
                                            </ol>
                                        </div>
                                    </div>

                                    <p>
                                        Combining yields the <i>Rayleigh quotient iterations</i>:
                                    </p>

                                    $$
                                    \begin{align}
                                    & \mathbf{for}\; k = 0, 1, 2, \dots \; \mathbf{do} \\
                                    & \qquad \mu^{(k)} \leftarrow r(v^{(k)}) \\
                                    & \qquad \text{Solve } (A - \mu^{(k)}I)w = v^{(k)} \\
                                    & \qquad v^{(k+1)} \leftarrow w / \|w\| \\
                                    & \mathbf{end\; for}
                                    \end{align}
                                    $$

                                    <p>
                                        which converges cubically for symmetric matrices and quadratically for general
                                        matrices.
                                    </p>

                </section>
            </article>

        </div>
    </main>


    <script src="../assets/js/navbar.js"></script>
    <script src="../assets/js/sidebar.js"></script>
    <script src="../assets/js/script.js"></script>
    <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
    <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>
</body>

</html>